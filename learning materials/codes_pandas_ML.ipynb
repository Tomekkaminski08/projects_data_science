{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>spis TREŚCI</b> ROZDZIAŁY\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [1. BIBLIOTEKI](#BIBLIOTEKI)\n",
    "2. [2. EDYCJA_TABEL](#EDYCJA_TABEL)\n",
    "3. [SORTOWANIE_FILTROWANIE](#SORTOWANIE_FILTROWANIE)\n",
    "4. [GRUPOWANIE](#GRUPOWANIE)\n",
    "5. [STATYSTYKA](#STATYSTYKA)\n",
    "6. [WYKRESY](#WYKRESY)\n",
    "7. [DATA_CLEANSING](#DATA_CLEANSING)\n",
    "8. [MACHINE_LEARNING](#MACHINE_LEARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk\\anaconda3\\python.exe\n",
      "C:\\Users\\tk\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BIBLIOTEKI'></a>\n",
    "# 1. BIBLIOTEKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25488/3957342806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m      \u001b[1;31m# LASY LOSOWE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline                                       # pozwala uruchamiać wykresy w środowisku\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression        #LinearRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.tree as tree                              # DRZEWO DECYZYJNE\n",
    "import sklearn  # each module in sklearn is separately imported, but it's nice to access it directly\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection                          #metryki do przygotowywania danych tren i test\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree                       # DRZEWO DECYZYJNE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, roc_auc_score,plot_roc_curve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder          #OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate      # DO WALIDACJI SKROŚNEJ(KRZYŻOWEJ)\n",
    "from sklearn.model_selection import GridSearchCV      # DO WALIDACJI SKROŚNEJ(KRZYŻOWEJ)\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "#from sklearn.datasets import load_wine, load_diabetes\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier      # LASY LOSOWE\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from mlxtend import plotting\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris, load_digits, make_circles, make_moons\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BIBLIOTEKIi'></a>\n",
    "## 1.1. BIBLIOTEKIi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. **EDYCJA**_*TABEL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dane z datasets - zmiana formatu\n",
    "boston = sklearn.datasets.load_boston()\n",
    "print(boston.keys())\n",
    "print(type(boston.data), boston.data.dtype, boston.data.shape)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " **reset_index** - resetuje index do 0-nieskończoności\n",
    "\n",
    "df.info(verbose=True)<br>\n",
    "<span style=\"color:red\">tekst</span><br>\n",
    "<span style=\"font-family:Comic Sans MS\">Inna czcionka</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A           zmieniamy MAKE na object\n",
    "dfv.MAKE = dfv.MAKE.astype(str)\n",
    "dfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd \"D:\\GIT HUBy\\00 praca domowa 5 7 pandas\\7 z ML\"\n",
    "df = pd.read_csv('train.csv')\n",
    "df.columns\n",
    "iris.keys()            # pokazuje źródła danych dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# C         liczy ile jest przypadków każdego rodzaju w kolumnie. Dodajac [0] na końcu wybieram konkretnie 1 kolumnę\n",
    "from collections import Counter\n",
    "Counter(df_copy['education']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# C\n",
    "app_table=df[['INJSEV', 'MODELYR', 'MAKE','SEATPOS', 'SEX', 'AGE','MANUSE']].copy()\n",
    "app_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# C \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# I\n",
    "dfo['AGE'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# I     sprawdzenie braków danych\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mozemy wybrac zakres wierszy i konkretne kolumny\n",
    "df.loc[0:5, ['LotArea', 'LotShape']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['AGE','CASENO','CHTYPE','WEIGHT','CASEID','PSU','YEAR'],inplace=True, ignore_index=True)\n",
    "df.drop_duplicates(inplace =True, ignore_index=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# przypisac nowe wartosci\n",
    "df['PoolArea'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# D usuwanie rekordów\n",
    "dfv.drop(dfv[dfv.MODELYR == 1930].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# D USUWANIE KOLUMN\n",
    "df_small = df.drop('newcol1', axis=1)\n",
    "# LUB\n",
    "df_small.drop(['newcol2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# F\n",
    "df.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#G   zmienna fikcyjna, w przypadku anych tekstowych, tworzymy kolumny pomocniecze(będą idealnie odwrotnie skorelowane)\n",
    "df_new = pd.get_dummies(df)  #dodane nowe kolumny_uczenie maszynowe leksykon str 30 a później:\n",
    "df_new = pd.get_dummies(df,drop_first=True) #teraz nowy df z wektore dla cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# wstawianie do pustych daych w kolumnie NaN jakiejś wartości konkretnej\n",
    "df=['education'] = df['education'].fillna('university.degree')\n",
    "df_copy['campaign'] = df_copy['campaign'].fillna(campaign_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mozemy wybrac zakres wierszy i zakres kolumn\n",
    "df.loc[0:5, 'MSSubClass':'LotShape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Nowa kolumna z obliczenia\n",
    "df.['NEW']=df['MSSubClass'] * df['LotFrontage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# NOWE KOLUMNY\n",
    "df_small['NEW1'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ale nadal mozemy jej uzywac przy innych operacjach\n",
    "df['NEW2'] = df.MSSubClass * df.LotFrontage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# S sprawdzamy ilosc rekordow i kolumn\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ZMIANA NAZWY KOLUMNY\n",
    "df_small = df_small.rename(columns={'NEW1' : 'Nowa Nazwa', 'NEW2' : 'Nowa Nazwa 2'})\n",
    "# METODA 2 (podobna)\n",
    "df_small.rename(columns = {'Nowa Nazwa':'new_col1', 'Nowa Nazwa 2':'new_col2'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pokazuje jakie są przypadki w kolumnie + PODAJE TYP\n",
    "df['education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# V        sprawdzamy ile razy dana wartosc powtarza sie w kolumnie\n",
    "df['MSZoning'].value_counts(dropna=False).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sprawdzamy ostatnie 5 rekordow (default) --> mozna zmienic dodajac liczbe w nawiasie\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.target   #ramka danych do przewidywania targetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Z zaokrąglanie\n",
    "np.round(pca.explained_variance_ratio_, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #Procedurą, którą należałoby zastosować jest odcięcie wartości odstających - da to pewność, że większość danych znajdzie się w oczekiwanym zakresie danych. \n",
    "# Najprostszym sposobem jest odcięcie wartości wzdłuż każdej osi, które nie mieszczą się w określonym zakresie.\n",
    "# dwie metody - poniżej opisane jest odcinanie wartości leżących powyżej kilku \n",
    "# odchyleń standardowych w naszym zestawie danych oraz metoda oparta o dystans między kwartylami (dla przypomnienia, \n",
    "# pierwszy i trzeci kwartyl to wartości, które są większe lub równe 25% i 75% danych).\n",
    "# Istnieją także bardziej zaawansowane metody odcinania wartości odstających, opisane tu:\n",
    "#     https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "### Usunięcie wartości odstających - trzy odchylenia standardowe\n",
    "\n",
    "def rm_out(df):                 #remove outliers \n",
    "    df_out = df.copy()         #kopia ramki danych\n",
    "    means, stdevs = {}, {}  # srednie i odchylenia standard\n",
    "                                                          # poniższe pętele stosowane dla liczb\n",
    "    for column in df_out.columns:              # chcę dla kolumn policzy średnią i standard odcylenie \\ dla kolumn dla df\n",
    "        means[column] = df_out[column].mean()          #tworzę słownik liczę średnią\n",
    "        stdevs[column] = df_out[column].std()          #tworzę słownik liczę odchylenie\n",
    "\n",
    "    for column in df_out.columns:           # dla wartosci ze słowników, dla każdej kolumny liczę dolną i gorną granicę  = średnia z danej kolumny odjać trzy odchylenia standard      \n",
    "        lower = means[column] - 3 * stdevs[column]  #śrenia danej kolumny - odchylenie\n",
    "        upper = means[column] + 3 * stdevs[column]  #dla wartosci ze słowników, dla każdej kolumny liczę dolną i gorną granicę  = średnia z danej kolumny plus trzy odchylenia standar\n",
    "        index = (lower < df_out[column]) & (df_out[column] < upper)  # moje wartości musza mniejsze niż granica która wyznaczam(indeksy)-granica dolna musi być mniejsza niż moje wartości\n",
    "        df_out = df_out[index]                                      # data frame po usunięcie, pozostaja tylko nieodstajace wartosci\n",
    "    df_out.index = range(len(df_out))  # robię iterację aby indeksy były w zakresie od 0\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1 SUB KOPIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_small = df[['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'MSZoning', 'SalePrice']].copy()\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#zapisane dane do nowej csv\n",
    "uber_analysis.to_csv('uber_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. SORTOWANIE_FILTROWANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check how many unique host_id\n",
    "total = len(data['host_id'].unique())\n",
    "percent = len(data['host_id'].unique())/data.shape[0] * 100\n",
    "print('Unique items in host_id is:{0}, it is {1:.2f}%'.format(total,percent))\n",
    "\n",
    "# Check how many unique id\n",
    "total = len(data['id'].unique())\n",
    "percent = len(data['id'].unique())/data.shape[0] * 100\n",
    "print('Unique items in id is:{0}, it is {1:.2f}%'.format(total,percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sortujemy przez konkretna kolumne (lub kolumny podane w liscie)\n",
    "df.sort_values(by='MSSubClass',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.unique(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# z COUNTER\n",
    "Counter(df_copy['education']).most_common()[0]\n",
    "Counter(df_copy['education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# liczenie średniej w kolumnie\n",
    "campaign_mean = np.mean(df_copy['campaign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filtrowanie po kolumnie, metoda 1\n",
    "df[df.MSSubClass == 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['Glucose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filtrowanie po kolumnie, metoda 2 z loc\n",
    "df.loc[df['MSSubClass'] == 180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# L lista kolumn\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# oczywiscie mozemy dodac wiecej kryteriow\n",
    "df[(df.MSSubClass == 180) & (df['LotArea'] >= 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# metoda z loc\n",
    "df.loc[(df['MSSubClass'] == 180) & (df['LotArea'] >= 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"Category\"]==\"var1\",\"var2\"].isnan().sum()\n",
    "x=df.loc[df[\"Category\"]==\"var1\",\"var2\"].dropna()   #tak pobiera się dane dla wybranych kolumn | z dropna usuwa wartości nieznne\n",
    "  # liczę ile jest wartośi \n",
    "n=len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# P procenty\n",
    "dane['class'].value_counts()/len(dane)*100 jak cos to .value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# metoda z query\n",
    "df.query('MSSubClass == 180 & LotArea >= 2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# filtrowanie przy uzyciu contains\n",
    "df[df['SaleCondition'].str.contains(\"norm\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#aby odkryć wsztstkie wyniki outputu w linijce z print trzeba wpisać:\n",
    "dfbmi=df.groupby(['Age','BMI'])[['BMI']].count()\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(dfbmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. GRUPOWANIE\n",
    "```\n",
    "pd.DataFrame.groupby()\n",
    "```\n",
    "- W .groupby() podajesz nazwe kolumnie po ktorej chcesz grupowac. Nastepnie [\"kolumna\"] podajesz kolumne na ktorej chcesz wykonac operacje.\n",
    "- W .groupby() sortowanie odbywa sie automatycznie na kolumnie, po ktorej sortujemy. Mozna to zmienic dodajac sort=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sprawdzmy ile jest rekordow dla poszczegolnego 'MSZoning'\n",
    "df_small.groupby('MSZoning')['MSSubClass'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# bez sortowania\n",
    "df_small.groupby('MSZoning', sort=False)[\"MSSubClass\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_small[df_small.MSZoning == 'C (all)']\n",
    "# sprawdzmy...mozna i tak\n",
    "df_small[df_small.MSZoning == 'C (all)'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(data_pandas[data_pandas.Wiek>30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# oczywiscie mozemy podac wiecej kolumn, po ktorych chcemy grupowac\n",
    "df_small.groupby(['MSZoning', 'MSSubClass'])[['LotArea', 'SalePrice']].agg([min, max, sum, 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "occupants_cat[(dfo.ROLE== 1) & (occupants_cat['INJSEV'] == 4)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfo.query('INJSEV == 4 | INJSEV ==2 & ROLE == 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#wybranie kolumny z wartościami do podmiany\n",
    "dfo['AGE'].value_counts()\n",
    "dfo[dfo.AGE== 20] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wypadki_per_role=occupants_cat.groupby([\"ROLE\"])['ROLE'].count()/occupants_cat.shape[0]\n",
    "wypadki_per_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q_table2.groupby('MANUSE')['MANUSE_Q'].agg([sum,'mean'])           #•\tniezapięte (0, 1) --> 1•\tzapięte (2 -18) --> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ROW_NUMBER ()over (partition by..order by)\n",
    "dfo['exam']=dfo.sort_values(['MANUSE','ROLE'],\\\n",
    "    ascending=[False,True])\\\n",
    "    .groupby['INJSEV','SEATPOS']\\\n",
    "    .cumcount()+1\n",
    "dfo.exam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# liczenie średniej dla grupy po zgrupowaniu\n",
    "body.groupby(['BODYTYPE'])[['CARAGE','BODYTYPE_sum', ]].agg([ sum, 'mean'])\n",
    "\n",
    "round((df.groupby('season'['height'].mean()),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bb=l_trend['bagfail_ratio'] = l_trend['BAGFAIL_sum'] / l_trend.groupby('YEAR')['BAGFAIL_sum'].transform('sum')\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dead_driver.groupby('AGE')['OCCNO'].sum().sort_values(ascending=False).plot(kind='bar', figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# korelacja\n",
    "cM= occupants_cat.corr()\n",
    "cM\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10,8)})            # okrślenie rozmiaru rysunku\n",
    "color_map = sns.diverging_palette(240, 10, n=10) # wybór mapy kolorów\n",
    "mask = np.triu(np.ones_like(cM), k=0)            # maska - ukrycie górnej macierzy trójkątnej\n",
    "sns.heatmap(cM,vmin=-1.,vmax=1.,cmap=color_map,mask=mask,square=True) # wykonanie wykresu\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Średnia po kategorii + podanie skrajnych wartości\n",
    "df.groupby(\"room_type\").agg(minimum_nights=(\"minimum_nights\",\"min\"),maximum_night=(\"minimum_nights\",\"max\"),average_price=(\"price\",\"mean\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "explode = [0, 0, 0, 0, 0.2,0,0]\n",
    "df.groupby(['INJSEV']).count().plot(kind='pie', y='CASENO',\n",
    "                                          explode=explode, autopct='%1.0f%%', figsize=(8, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [30, 5])\n",
    "\n",
    "# Pregnancies\t\n",
    "plt.subplot(1, 4, 1)\n",
    "g = sns.countplot(data=df, x=\"Pregnancies\", order=df.Pregnancies.value_counts().index)\n",
    "g.set_xlabel('Age')\n",
    "g.set_ylabel('Pregnancies')\n",
    "\n",
    "# Glucose\tBloodPressure\tSkinThickness\tInsulin\tBMI\tDiabetesPedigreeFunction\n",
    "plt.subplot(1, 4, 2)\n",
    "g = sns.countplot(data=df, x=\"Glucose\", order=df.Glucose.value_counts().index)\n",
    "g.set_xlabel('Age')\n",
    "g.set_ylabel('Glucose')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# histogram dla wszystkich kolumn\n",
    "df.hist(figsize = (15,9), color='midnightblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=df[((df['MANUSE'] == 1) | (df['BloodPressure'] == 2))],\n",
    "            x=\"BMI\",\n",
    "            y=\"BloodPressure\",\n",
    "            aspect=2, \n",
    "            hue='BloodPressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=df,x='Age', kind='count',aspect=2.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1. USUNIĘCIE_WARTOŚCI_ODSTAJĄCYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20388/3247899402.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\tk\\AppData\\Local\\Temp/ipykernel_20388/3247899402.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <a id='USUNIĘCIE WARTOŚCI ODSTAJĄCYCH'></a>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<a id='USUNIĘCIE WARTOŚCI ODSTAJĄCYCH'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [44, 6])\n",
    "plt.subplot(1, 2, 1)\n",
    "g = sns.countplot(data=df, x=\"Age\", hue=\"Outcome\", order=df.Age.value_counts().index)\n",
    "g.set_xlabel('Age')\n",
    "g.set_ylabel('Outcome');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.groupby('Age')['Pregnancies'].sum().plot(kind='bar', figsize=(20,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. STATYSTYKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #PRZEDZIAŁY UFNOSCI\n",
    "x= df.loc['category']=='cos1','cos2']\n",
    "mean = tmean(x)   #scipy stats\n",
    "n=len(x)\n",
    "s=st.tstd(x)\n",
    "a = 1-0.99\n",
    "RN = st.norm(0,1)\n",
    "u = RN.ppf(1-a/2)           #kwantyl\n",
    "CI = [mean - s/np.sqrt(n)*u, m + s/np.sqrt(n)*u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #bootstrap - POBIERANIE DANYCH I LICZENIE ŚREDNIEJ\n",
    "MEAN = list()\n",
    "for i in range(1000):\n",
    "    MEAN.append(st.tmean (choice (x,size=len(x), replace=True)    #...(x[::2],size=len(x)   ŚREDNIA CO DRUGIEGO ELEMENTU\n",
    "MEAN np.array (MEAN) \n",
    "plt.hist (MEAN,bins=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. WYKRESY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pasy_vs_obrazenia=occupants_cat.groupby([\"MANUSE\",\"INJSEV\"])[\"MANUSE\",\"INJSEV\"].count()\n",
    "# pasy_vs_obrazenia\n",
    "pasy_vs_obrazenia2=occupants_cat.groupby([\"MANUSE\",\"INJSEV\"])[\"MANUSE\"].count()\n",
    "pasy_vs_obrazenia2\n",
    "\n",
    "pasy_vs_obrazenia2.sum()\n",
    "\n",
    "prawodpodobienstwo_obrazen_vs_pasy=pasy_vs_obrazenia2/pasy_vs_obrazenia2.sum()\n",
    "\n",
    "prawodpodobienstwo_obrazen_vs_pasy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(X['neighbourhood_group_l'],df['price'],n_levels=15,cbar=True,shade=True,shade_lowest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std_min_max(series):         # LICZENIE ŚRENIEJ I ODCHYLEŃ PRZY POMOCY NUUMPY\n",
    "    # look up: https://pyformat.info/ - great source on string formatting in Python\n",
    "    return \"(mean: {:f}, std: {:f}, min: {:f}, max: {:f})\".format(\n",
    "        series.mean(),\n",
    "        series.std(),\n",
    "        series.min(),\n",
    "        series.max()\n",
    "    )\n",
    "    \n",
    "for series_index in range(boston.data.shape[1]):\n",
    "    print(boston.feature_names[series_index], get_mean_std_min_max(boston.data[:, series_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#określenie prawdopodobieństw\n",
    "pk = list(map(lambda x: x[-1],P))  #obliczanie częstości\n",
    "for e in P:\n",
    "    pk.append(e[-1])\n",
    "pk=np.array(pk)\n",
    "pk=pk/np.sum(pk)\n",
    "                  rozklad = st.rv_discrete(name='Rozkład z ćw.', values = (xk, pk))\n",
    "                   rozk = st.rv_discrete(values=(xk,pk))\n",
    "                   rozk.expect()\n",
    "# rysowanie wykresu zmiennych losowych dysktretnych\n",
    "p=rozk.pmf(xk)\n",
    "F=rozk.cdf(xk)\n",
    "plt.vlines(xk,0,p,colors='r', lw=4)\n",
    "plt.hlines(np.append([0]), lw = 2)\n",
    "\n",
    "#prawdopodobiństwo że zmienna losowa opisana rozkładem Poissona przyjmie wartość między 2 a 5\n",
    "P = rv.cdf(5) - rv.cdf(2)\n",
    "\n",
    "                          #ZADANIE\n",
    "    #tworzymy rozkład normalny i liczę gętośc rozkładu prawdopodbieństwa\n",
    "mean = 5  #srednia albo wartość oczekiwan\n",
    "odchstand = 0.25\n",
    "RN = st.norm(mena,odchstand)\n",
    "x = RN.rvs(size = 1000)       # x to dane wczytane z tabeli\n",
    "parameters = st.norm.fit(x)   #znajdowanie param(jako krotka)rozkładu metodą najbardziej wiarygodnej, fit umożliwia znalezienie parametrów dowolnego z rozkładów\n",
    "RNdane - st.norm(*p)          #rozbicie każdego elemntu krotki lub listy oddzielnie st.norm(p[0],p[1])\n",
    "plt.hist(x,bins=12,density=True,edgecolor='black')\n",
    "t=np.arange(min(x),max(x),0.01) #generowanie wartości iksów\n",
    "plt.plot(t,RNdne.pdf(t))        #dwa wykresy rozkładu i próby muszą być podobne. PDF do ciagłych\n",
    "# kolejny krok to funkcja licząca dwa różne rozkłądy\n",
    "R = [st.expon, st.norm]\n",
    "\n",
    "plt.hist(x,bins=15, density=True, edgecolor=\"black\")\n",
    "for d in R:\n",
    "    p = d.fit(x)\n",
    "    rozk = d(*p) \n",
    "    t =np.arange(min(x),max(x),0.01) \n",
    "    plt.plot(t, rozk.pdf(t))\n",
    "RNdane.ppf(0.8)  # taki provent ma mniejszy udział KWANNTYL 0- wartość oddzieljąca pola na wykresie\n",
    "\n",
    "#Narysuj gęstość rozkładu prawdopodobieństwa i dystrybuantę w zakresie $[\\mu-4\\sigma;\\mu+4\\sigma]$;\n",
    "m=125\n",
    "s=15\n",
    "NormalDistribution = st.norm(m,s)\n",
    "x = np.linspace(m-4*s, m+4*s, 100)\n",
    "f = NormalDistribution.pdf(x)  #funkcja gęstości rozkładu prawdopodbiensta\n",
    "F = NormalDistribution.cdf(x)  #obliczenie wartości dystrybuanty\n",
    "plt.plot(x,f,'blue',x,F,'red')\n",
    "plt.legend(['f(x) - gęstość','F(x) - dystrybuanta'])\n",
    "# odchylenie stanardowe\n",
    "S = NormalDistribution.std()\n",
    "#kwartyl q025\n",
    "q025 = NormalDistribution.ppf(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo=pd.read_csv(\"./occupants_cat_z_bagfail.csv\")\n",
    "dfo['STRATIF'] = pd.Categorical(dfo['STRATIF'], \n",
    "                                                categories=[1,2,3,4,5], \n",
    "                                                ordered=True)\n",
    "plt.figure(figsize=(9,8))\n",
    "plt.suptitle('Zależność rodzaju zdarzenia od rocznika samochodu', fontsize=14, fontweight='semibold')\n",
    "\n",
    "# heatmap dla role1\n",
    "plt.subplot(1, 2, 1)\n",
    "dfa = dfo.query('ROLE == 1').groupby([\"HEIGHT\", \"STRATIF\"])[\"PSU\"].size().reset_index()          # ('user_type == \"Customer\"')\n",
    "dfa = dfa.pivot(\"HEIGHT\", \"STRATIF\",\"PSU\")\n",
    "sns.heatmap(dfa, cmap=\"BuPu\")\n",
    "\n",
    "plt.title(\"ROLE1\", y=1.015)\n",
    "plt.xlabel(\"STRATIF\")\n",
    "plt.ylabel(\"HEIGHT\")\n",
    "\n",
    "# heatmap dla role2\n",
    "plt.subplot(1, 2, 2)\n",
    "dfp = dfo.query('ROLE == 2').groupby([\"HEIGHT\", \"STRATIF\"])[\"PSU\"].size().reset_index()\n",
    "dfp = dfp.pivot(\"HEIGHT\", \"STRATIF\",\"PSU\")\n",
    "sns.heatmap(dfp, cmap=\"BuPu\")\n",
    "\n",
    "plt.title(\"ROLE2\", y=1.015)\n",
    "plt.xlabel(\"STRATIF\")\n",
    "plt.ylabel(\"HEIGHT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. WIZUALIZACJA STATYSTYCZNA + jądrowy estymator do określenia prawdopodobieństwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pudełko\n",
    "sns.set(rc={'figure.figsize':(22,8)}, font_scale=1.5, style='whitegrid')\n",
    "df[df.columns[np.concatenate([[1],list(range(1,8))])]].plot.box() # wykres skrzynkowy udziału \n",
    "pass\n",
    "# histogram - najpierw tworzę zmiennądo jednej kolumh\n",
    "d = df[\"BMI\"].to_numpy()\n",
    "plt.hist(d,bins=18,edgecolor='black')\n",
    "#rozkład - jądrowy estymator\n",
    "kde = st.gaussian_kde(d)\n",
    "t = np.arrange(min(d),min(d),1.0)\n",
    "plt.plot(t.kde(t))\n",
    "   # określam prawdopodobieństwo trafienia na potraweę 200-300 kalorii\n",
    "a=200 b=300\n",
    "p=kde.integrate_box_1d(a,b)  #STATYSTYKA 1 02:26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. DATA CLEANSING KZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                  # STRUKTURA DANYCH\n",
    "# popatrz na strukturę danych - ilość rekordów, ilość kolumn, jakiego rodzaju dane zawierają\n",
    "# (tekst, liczby, cechy) i jak będziesz w związku z tym z nimi pracować\n",
    "np.unique(x)  # x to nazwa kolumny\n",
    "df.isnull().sum()\n",
    "                                                # DYSTRYBUCJA WARTOŚCI\n",
    "# sprawdź dystrybucję różnych wartości w zależności od cechy - pomogą w tym histogramy oraz percentyle\n",
    "# dowiesz się dzięki temu, w jakim rozkładzie są Twoje zmienne\n",
    "\n",
    "                          # 1 kasuję outliery\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "x=[25,33,45,27,63,18,25,22,31,36]\n",
    "Q1, Q3 = np.percentile(x,[25,75]) # licząc percentyle podaje zbiór oraz wybrane percentyle 0,25 i 0,75\n",
    "Q1, Q3\n",
    "[Q1 - 1.5 * iqr(x), Q3 + 1.5 * iqr(x)]\n",
    "                           # 1 kasuję outliery - filmik z you tube https://www.youtube.com/user/Mobilo24eu/videos\n",
    "plt.figure() \n",
    "sns.boxplot (wine['alcohol']) plt.plot()\n",
    "from scipy import stats\n",
    "z = np.abs (stats.zscore (wine))\n",
    "print(z)\n",
    "threshold = 4\n",
    "print(np.where(z > threshold))\n",
    "                             # removing outliers with Z-score\n",
    "wine_o_z = wine [(z<threshold).all(axis=1)]\n",
    "                             # detecting outliers with IQR method\n",
    "Q1 wine.quantile (0.25) Q3 wine.quantile (0.25)\n",
    "Q3 wine.quantile (0.25) Q3 wine.quantile (0.75)\n",
    "IQR = Q3 - Q1\n",
    "((wine (Q1 1.5 IQR)) | (wine (Q3 +1.5 * IQR)))\n",
    "((wine (Q1 1.5 IQR)) | (wine (Q3 +1.5 IQR))).iloc[14:18,4:7]\n",
    "                             #removing outliers\n",
    "outlier_condition = ((wine < (Q1 -1.5* IQR)) | (wine > (Q3 +1.5 * IQR))) \n",
    "wine_o_iqr =wine [~outlier_condition.any (axis=1)] \n",
    "wine_o_iqr\n",
    "wine[\"quality\"].unique()\n",
    "wine_o_z[\"quality\"].unique() \n",
    "wine_o_iqr[\"quality\"].unique()\n",
    "\n",
    "for i in wine.columns[:-1]:\n",
    "    sns.boxplot(x=wine ['quality'], y=wine[i], ax=axs[0]) \n",
    "    sns.boxplot(x=wine_o_z[ 'quality'], y=wine_o_z[i], ax=axs[1]) \n",
    "    sns.boxplot (x=wine_o_iqr['quality'], y=wine_o_iqr[i], ax=axs[2]) \n",
    "    plt.plot()\n",
    "\n",
    "columns ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar\",'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "X = wine [columns]\n",
    "y = wine['quality'].astype (float)\n",
    "X = wine_o_z[columns]\n",
    "y = wine_o_z['quality'].astype(float)\n",
    "X = wine_o_iqr[columns]\n",
    "y = wine_o_iqr['quality'].astype(float)\n",
    "scaler= StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test= train_test_split(x, y, test_size=0.2)\n",
    "lr =Linear Regression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred =lr.predict(X_test)\n",
    "good_counter np.count_nonzero(y_test == np.rint(y_pred)) \n",
    "total_counter len (y_test) \n",
    "print(good counter / total counter)____________________________________________________________________________________\n",
    "                                                    # DANE ILOŚCIOWE - CECHY STATYSTYCZNE\n",
    "# dla danych ilościowych - dla każdej zmiennej osobno, albo przynajmniej dla najważniejszych - zerknij na\n",
    "# średnie, mediany, średnie odchylenia i wariancje - upewnisz się, czy w danych da się zauważyć pewne tendencje\n",
    "\n",
    "# DANE ILOŚCIOWE - SENS DANYCH\n",
    "# podobnie, jeżeli znasz merytoryczny sens zmiennej, sprawdź czy zgadza się z tym, co widzisz.\n",
    "# np. wartość prędkości samochodu w tysiącach km/h, czy cena mieszkań wyraźnie odbiegająca od tego czego się spodziewasz\n",
    "____________________________________________________________________________________________\n",
    "# DANE JAKOŚCIOWE - KATEGORIE I REPREZENTACJA\n",
    "# dla danych jakościowych sprawdź ilość kategorii oraz reprezentację liczbową każdej z nich dla konkretnej, badanej cechy - tutaj również pomogą histogramy\n",
    "sns.pairplot(df)\n",
    "\n",
    "___________________________________________________________________________________________________\n",
    "# WARTOŚCI NIEPOPRAWNE\n",
    "# wyłap wartości ewidentnie niepoprawne (np. NULL, tekst zamiast liczby, minus w liczbach\n",
    "# dodatnich) i augmentuj brakujące (lub usuń rekordy, jeśli to lepsze wyjście)\n",
    "\n",
    "# KORELACJE\n",
    "# sprawdź, czy zachodzą korelacje, czy któreś cechy nie są ze sobą wyraźnie powiązane i zastanów się, z czego to może wynikać/ może je usuń\n",
    "correlation_matrix = data[feature_list].corr()       #ML1 02:15\n",
    "np.abs(cM) >= 0.8\n",
    "sns.set(rc={'figure.figsize':(10,8)}) # okrślenie rozmiaru rysunku\n",
    "color_map = sns.diverging_palette(240, 10, n=10) # wybór mapy kolorów\n",
    "mask = np.triu(np.ones_like(cM), k=0) # maska - ukrycie górnej macierzy trójkątnej\n",
    "sns.heatmap(cM,vmin=-1.,vmax=1.,cmap=color_map,mask=mask,square=True) # wykonanie wykresu, wyskalowanie od -1 do 1\n",
    "pass\n",
    "\n",
    "# TRANSFORMACJA DANYCH\n",
    "#zastosuj normalizację, standaryzację i one-hot encoding, aby uniknąć niezamierzonego “skrzywienia” modelu podczas uczenia\n",
    "\n",
    "# WIZUALIZACJE\n",
    "# wizualizuj wszystkie kroki - to, co widoczne na odpowiednich wykresach, nie zawsze jest widoczne w samych liczbach\n",
    "\n",
    "np.unique(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.1.1. Analiza tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfc1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "train_data.shape, test_data.shape   # sprawdzam proporcje obu zbiorów, domyślny stosunek to 0,25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                 # podział na dane testowe i treningowe\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_new[features], df_new[target], random_state=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(dfc1.drop('caramel', axis=1), dfc1['caramel'])\n",
    "             #test size to proporcja podziału zbioru\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.3,random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                               # Sprawdzenie braków danych\n",
    "def check_missing():\n",
    "    for column in df.columns:\n",
    "        missing = column, df[column].isnull().sum()\n",
    "        if missing[1] == 0: continue\n",
    "        print(missing)\n",
    "check_missing()\n",
    "missing_perc =( (df.isnull().sum()/df.shape[0]) * 100 ).sort_values(ascending=False)\n",
    "missing_perc.plot.bar(figsize=(10,5))\n",
    "plt.title(\"Brakujace dane [%]\", y=1.01)\n",
    "plt.hlines(xmin=df.index.min(), xmax=df.index.max(), y=1, color='r', linestyle='-.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.1.2. INŻYNIERIA CECH wg ML1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20388/1115342645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# ONE HOT ENCODING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# x = df[\"nazwa_kolumny\"].to_numpy()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#lista-transpozycja do kolumny\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmy_encoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#Utworzenie obiektu klasy OneHotEncoder wykonuje się poprzez konstruktor klasy `OneHotEncoder`. Domyślnie tworzona jest macierz rzadka, tu w celu lepszej ilustracji zagadnienia wymuszono utworzenie mcierzy pełnej poprzez podanie argumentu `sparse=False`-NIE OPTYMALIZUJ DANYCH!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "                                         # ONE HOT ENCODING\n",
    "# x = df[\"nazwa_kolumny\"].to_numpy()\n",
    "x = np.array([\"a\",\"b\",\"c\",\"a\",\"c\",\"a\",\"c\",\"b\",\"c\",\"a\",\"b\"],ndmin=2).transpose() #lista-transpozycja do kolumny\n",
    "x.shape\n",
    "my_encoder = OneHotEncoder(sparse=False)  #Utworzenie obiektu klasy OneHotEncoder wykonuje się poprzez konstruktor klasy `OneHotEncoder`. Domyślnie tworzona jest macierz rzadka, tu w celu lepszej ilustracji zagadnienia wymuszono utworzenie mcierzy pełnej poprzez podanie argumentu `sparse=False`-NIE OPTYMALIZUJ DANYCH!\n",
    "my_encoder.fit(x)                         #W chwili obecnej obiekt `my_encoder` jest \"pusty\". nie zawiera żadnych danych. W celu jego pełnego określenia należy użyć metody `fit`, a jako argumentu użyć tablicy z kategoriami.\n",
    "my_encoder.categories_                    #W celu sprawdzenia poprawności kodowania należy wypisać pole `categories_` instancji `my_encoder` klasy `OneHotEncoder`:\n",
    "xohe = my_encoder.transform(x)            #W celu dokonania transformacji danych zgodnie z zasadami określonymi przez my_encoder należy użyć metody `transform`. Poniżej zastosowanie tej metody do tablicy x, na podstawie, której utworzono enkoder:\n",
    "xohe\n",
    "         #Poniżej wykonano kodowanie dla nowych danych (np. zbiór testowy):\n",
    "x_nowy = np.array([\"b\",\"b\",\"c\",\"a\"],ndmin=2).transpose()\n",
    "x_nowy\n",
    "x_nowy_ohe = my_encoder.transform(x_nowy) #Transformacja nowych danych.**Uwaga:** W przypadku wprowadzania nowych danych należy się upewnić, czy w nowych danych nie występują nowe wartości np. etykieta `d`\n",
    "x_nowy_ohe\n",
    "# OneHotEncoder może działać na więcej niż jednej kolumnie. W takim wypadku utworzony zostanie enkoder dla każdej kolumny. Poniżej zmienna `X` zawiera trzy cechy: pierwsza cecha ma unikane wartości `a`, `b` oraz `c`, druga cecha ma unikalne wartości całkowite `1`, `3`, `4`, a trzecia cecha ma unikalne wartości `u`, `v` oraz `z`.\n",
    "X = np.array([[\"a\",\"b\",\"c\",\"a\",\"c\",\"a\",\"c\",\"b\",\"c\",\"a\"],\n",
    "              [ 1 , 4 , 5 , 1 , 5 , 4 , 3 , 1 , 3 , 1 ],\n",
    "              [\"u\",\"z\",\"z\",\"v\",\"v\",\"u\",\"z\",\"v\",\"u\",\"z\"]]).transpose()\n",
    "X.shape\n",
    "my_encoder1 = OneHotEncoder(sparse=False)  #Utworzenie enkodera i wypisanie określonych kategorii\n",
    "my_encoder1.fit(X)\n",
    "my_encoder1.categories_\n",
    "my_encoder1.transform(X)\n",
    "_________________________________________________________________________________________________________________________\n",
    "                                    # SKALOWANIE -MINMAXSCALINER\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x = np.array([0.1, 10., 15., -5.1, 6.2],ndmin=2).transpose()\n",
    "x.shape\n",
    "my_scaler = MinMaxScaler()                  #W chwili obecnej obiekt `my_scaler` jest \"pusty\" i nie zawiera żadnych danych. W celu jego pełnego określenia należy użyć metody `fit`, a jako argumentu użyć macierzy z wartościami.\n",
    "my_scaler.fit(x)\n",
    "[my_scaler.data_min_, my_scaler.data_max_]  #W pełni określony scaler posiada pola charakteryzujące dane wejściowe, np. wartość maksymalną i minimalną:\n",
    "xs = my_scaler.transform(x)                 #W celu dokonania transformacji danych zgodnie z zasadami określonymi przez `my_scaler` należy użyć metody `transform`. Poniżej zastosowanie tej metody do tablicy x, na podstawie, której utworzono scaler:\n",
    "xs\n",
    "# MinMaxScaler może działać na więcej niż jednej kolumnie, wówczas skaluje każdą cechę niezależnie\n",
    "X = np.array([[ 2. , 6.  , 200.  , -450.,  105.  , 344.1, -33.],\n",
    "              [ 0.1, 0.01,   0.02,    0.02,  0.01,   0.04,  0.032]]).transpose()\n",
    "my_scaler1 = MinMaxScaler()\n",
    "my_scaler1.fit(X)\n",
    "[my_scaler1.data_min_, my_scaler1.data_max_]\n",
    "\n",
    "Xs = my_scaler1.transform(X)\n",
    "Xs\n",
    "# Skalowane kolumn do różnych zakresów. Poniżej utworzono dwa różne slcalery i przeskalowano każdą kolumnę oddzielnie, a następnie dokonano konkatenacji nowych kolumn.\n",
    "my_scalerA = MinMaxScaler(feature_range=(-1,1))\n",
    "my_scalerB = MinMaxScaler(feature_range=(-10,10)) # ZAKRES PODANY W KROTCE\n",
    "my_scalerA.fit(X[:,0].reshape((-1,1)))           # KAŻDA KOLUMNA SKALOWANA OSOBNO\n",
    "my_scalerB.fit(X[:,1].reshape((-1,1)))\n",
    "_________________________________________________________________________________________________________________________\n",
    "                                    # STANDARYZACJA -MINMAXSCALINER\n",
    "#Standaryzacja polega na odjęciu od każdego elementu cechy jej średniej i podzieleniu przez odchylenie standardowe, zgodnie ze wzorem: \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = np.array([0.1, 10., 15., -5.1, 6.2],ndmin=2).transpose()\n",
    "x.shape\n",
    "my_scaler = StandardScaler()        # konstruktor\n",
    "my_scaler.fit(x)\n",
    "[my_scaler.mean_, my_scaler.var_]  #W pełni określony scaler posiada pola charakteryzujące dane wejściowe, np. średnią oraz wariancję:\n",
    "xs = my_scaler.transform(x)        #dokonania transformacji danych\n",
    "#Poniżej wykonano skalowanie dla nowych danych (np. zbiór testowy):\n",
    "x_nowy = np.array([5., 2., -6, 16.],ndmin=2).transpose()\n",
    "x_nowy_s = my_scaler.transform(x_nowy)\n",
    "            # MinMaxScaler może działać na więcej niż jednej kolumnie, wówczas skaluje każdą cechę niezależnie\n",
    "X = np.array([[ 2. , 6.  , 200.  , -450.,  105.  , 344.1, -33.],\n",
    "              [ 0.1, 0.01,   0.02,    0.02,  0.01,   0.04,  0.032]]).transpose()\n",
    "my_scaler1 = StandardScaler()\n",
    "my_scaler1.fit(X)\n",
    "[my_scaler1.mean_, my_scaler1.var_]\n",
    "Xs = my_scaler1.transform(X)\n",
    "_________________________________________________________________________________________________________________________\n",
    "\n",
    "X_standarized=pd.DataFrame()\n",
    "for column in X.iloc[:].columns:\n",
    "    X_standarized[column] = X[column].astype(float)\n",
    "\n",
    "X_standarized.loc[:] = preprocessing.StandardScaler().fit_transform(X_standarized.loc[:])\n",
    "X_standarized\n",
    "_____________________-\n",
    "#set x and y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = dataset.iloc[:,0:11]\n",
    "y = dataset['quality']\n",
    "\n",
    "#stadardize data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "_______________________________________________________________________________________\n",
    "\n",
    "        #LABEL ENCODER\n",
    "df[\"quality\"] = df[\"quality\"].astype(object)\n",
    "bins = [2, 6.5, 8]\n",
    "values = [\"bad\", \"good\"]\n",
    "df[\"quality\"] = pd.cut(df[\"quality\"], bins=bins, labels= values)\n",
    "df[\"quality\"].unique()\n",
    "label_qlt = LabelEncoder()\n",
    "df[\"quality\"] = label_qlt.fit_transform(df[\"quality\"])\n",
    "df[\"quality\"].value_counts()\n",
    "             # PROSTY SPOSÓB\n",
    "# Zamieniam room_type na kategorie liczbowe - jest zachowany porządek w danych \n",
    "# (Entire home/apt > Private room > Shared room), więc zrobię to LabelEncoderem\n",
    "labelencoder = LabelEncoder()\n",
    "df['room_type_le'] = labelencoder.fit_transform(df['room_type'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.1.3. Wstępne kroki tworzenia modelu ML1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "                    #1 biblioteki podstawowe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "                      # biblioteki Scikit-learn\n",
    "from sklearn.model_selection import train_test_split # podział na zbiór testowy i treningowy\n",
    "from sklearn.metrics import mean_squared_error # funkcja obliczająca metrykę\n",
    "from sklearn.tree import DecisionTreeRegressor # estymator regresji=regresor\n",
    "from sklearn.preprocessing import LabelEncoder # funkcja do kodowania etykiet na liczby całkowite, ZAMIENIA LITERKI NA LICZBY\n",
    "from sklearn.model_selection import GridSearchCV # KROSWALIDACJA funkcja przeszukiwania przestrzeni parametrów i walidacji krzyżowej,WYBRANIE NAJLEPSZEGO MODELU\n",
    "\n",
    "#2 WYBIERAMY DANE, KLASYFIKUJEMY CO NAS INTERESUJE, USUWANY PUSTE DANE\n",
    "melbourne_data = pd.read_csv(\"melb_data.csv\")\n",
    "feature_list = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea','YearBuilt', 'Lattitude', 'Longtitude']  #LISTA IKSÓW - NIE POWNNY BYĆ SKORELOWANE\n",
    "target_label = 'Price'                                                                                   #LISTA YGREKÓW, z  nim muszą być skorelowwane iksy\n",
    "data = melbourne_data[feature_list+[target_label]].dropna()\n",
    "_________________________________________________________________________________________________________________________\n",
    "# MACIERZ KORELACJI - PO TYM USUWAM ZMIENNE KTÓRE WYSZŁY POWYŻEJ 0,8\n",
    "correlation_matrix = data[feature_list].corr()\n",
    "np.abs(cM) >= 0.8\n",
    "sns.set(rc={'figure.figsize':(10,8)}) # okrślenie rozmiaru rysunku\n",
    "color_map = sns.diverging_palette(240, 10, n=10) # wybór mapy kolorów\n",
    "mask = np.triu(np.ones_like(cM), k=0) # maska - ukrycie górnej macierzy trójkątnej\n",
    "sns.heatmap(cM,vmin=-1.,vmax=1.,cmap=color_map,mask=mask,square=True) # wykonanie wykresu\n",
    "pass\n",
    "# Trenowanie modelu\n",
    "# Zmiana cech i zeminnej celu na tablice numpy\n",
    "X = data[feature_list].to_numpy()            #tak działa scikit-learn\n",
    "y = data[target_label].to_numpy()\n",
    "____________________________________________________________________________________________________________________________-\n",
    "#3 Podział na zbiór testowy i treningowy\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=2021)  # zbiór sam dobiera wartości, aby mieć podobnie jaj inni -dajemy ziarno 2021\n",
    "__________________________________________________________________________________________________________________\n",
    "#4 Utworzenie i trenowanie modelu dla zbioru treningowego, korzystamy z modelu DRZEWA który ma dużo parametrów, ale w tym ćwiczeniu skupiamy się tylko na przepływie\n",
    "reg1 = DecisionTreeRegressor(random_state=2021)  #aby mieć podobnie jaj inni -dajemy ziarno 2021, w praktyce nie daje się\n",
    "reg1.fit(train_X, train_y)  \n",
    "\n",
    "# Obliczenie wartości błędu - Pierwiastka ze średniego błędu kwadratowego\n",
    "y_pred = reg1.predict(test_X)       # to co nas interesuje to zbiór testowy którego wcześniej nie widziałeś, i to ma sens\n",
    "rmse1 = np.sqrt(mean_squared_error(test_y, y_pred))               #średniego błędu kwadratowego pobiera y i y_predykcji\n",
    "rmse1                                                             #lub np.sqrt(mean_squared_error) aby mieć mniejszą liczbę\n",
    "plt.scatter(train_y,reg1.predict(train_X),edgecolor=\"black\",s=15) #rysuje wykres rozrzutu wartości - ten będzie nieidealnie dopasowany\n",
    "plt.xlabel(\"obserwacje\")\n",
    "plt.ylabel(\"predykcje\")\n",
    "plt.scatter(test_y,y_pred,edgecolor=\"black\",s=15)                #rysuje wykres rozrzutu wartości - ten będzie idealnie dopasowany\n",
    "plt.xlabel(\"obserwacje\")\n",
    "plt.ylabel(\"predykcje\")\n",
    "_____________________________________________________________________________________________________________________________\n",
    "#5 kolejne kroki gdy dodajemy zmienne - Dodanie kolejnej cechy - typ zabudowy 'Type' i tworzenie kolejnego modelu. Sprawdzenie czy poprawi to jakość predykcji.\n",
    "#**Uwaga**: W praktyce takie czynności automatyzuje się.\n",
    "feature_list = ['Type','Rooms', 'Bathroom', 'Landsize', 'BuildingArea','YearBuilt', 'Lattitude', 'Longtitude']\n",
    "target_label = 'Price'\n",
    "data = melbourne_data[feature_list+[target_label]].dropna()\n",
    "data\n",
    "    # kodowanie typu zabudowy na liczbę\n",
    "        my_label_encoder = LabelEncoder() # utworzenie encodera\n",
    "        my_label_encoder.fit(data[\"Type\"]) \n",
    "        my_label_encoder.classes_\n",
    "# dzielimy ponownie na zbiory        \n",
    "tmp = my_label_encoder.transform(data[\"Type\"]).reshape((-1,1)) # zamiana typu na liczbę - użycie enkodera\n",
    "X1 = np.concatenate((tmp,X),axis=1) # dodanie nowej kolumny do danych\n",
    "train_X, test_X, train_y, test_y = train_test_split(X1, y, random_state=2021) # podział na zbiór testowy i treningowy\n",
    "\n",
    "reg2 = DecisionTreeRegressor(random_state=2021) # utworzenie regresora\n",
    "reg2.fit(train_X, train_y) # trenowanie\n",
    "\n",
    "y_pred = reg2.predict(test_X)\n",
    "rmse2 = np.sqrt(mean_squared_error(test_y, y_pred))\n",
    "rmse2                                                 # tutaj ponownie rysujemy scatter plot jak powyżej\n",
    "__________________________________________________________________________________________________________________________\n",
    "# 6. Wykorzystanie [GridSearchCV] do polepszenia własności modelu. W przypadku modeli, które posiadają hiperparametry dobrym pomysłem jest zautomatyzowanie przeszukiwania przestrzeni hiperparametrów w celu znalezienie jak najlepszego modelu.\n",
    "#**Uwaga:** Parametry modelu i hiperparametry nie są tym samym.używamy grid search aby jeszcze znaleźć parametry\n",
    "parameters = {'min_samples_split':(2,3,4,6,8), 'max_features':(\"auto\",\"sqrt\",\"log2\")} # określenie słownika parametrów\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor(random_state=2021) # utworzenie regresora\n",
    "regCV = GridSearchCV(tree_regressor, parameters) # utworzenie obiektu przeszukiwania\n",
    "regCV.fit(train_X,train_y) # trenowanie\n",
    "\n",
    "# Wybór najlepszego regresora. Jest to najlepszy znaleziony regresor. Jeśli wyniki są niezadowalające, wówczas warto zmienić wartości parametrów i uruchomić przeszukiwanie ponownie.\n",
    "best_reg_tree = regCV.best_estimator_\n",
    "best_reg_tree\n",
    "y_pred = best_reg_tree.predict(test_X)\n",
    "\n",
    "rmseCV = np.sqrt(mean_squared_error(test_y, y_pred))\n",
    "rmseCV\n",
    "plt.scatter(test_y,y_pred,edgecolor=\"black\",s=15)       # i znów wykresy.....\n",
    "plt.xlabel(\"obserwacje\")\n",
    "plt.ylabel(\"predykcje\")\n",
    "\n",
    "# Porównanie RMSE modeli\n",
    "[rmse1, rmse2, rmseCV]\n",
    "\n",
    "# Analiza ważności cech\n",
    "#Niektóre modele (np. drzewa) umożliwiają łatwy dostęp do informacji, które cechy były najbardziej istotne w predyckji wartości wyjściowej. \n",
    "#Jest to bardzo przydatna informacja np. dla działu marketingu.\n",
    "best_reg_tree.feature_importances_\n",
    "\n",
    "plt.bar(feature_list,best_reg_tree.feature_importances_)\n",
    "plt.xticks(rotation=45)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![pobrane.png](https://regenerativetoday.com/simple-explanation-on-how-decision-tree-algorithm-makes-decisions/)\n",
    "![jupyter](img/pobrane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.2. ML2_REGRESJA LINIOWA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### REGRESJA LINIOWA YOU TUBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "animals = pd.read_csv(r\"C:\\data\\animals.csv\")\n",
    "animals = animals animals ['name'].isin(['Cow', 'Goat', 'Donkey', 'Horse', 'Giraffe', 'Kangaroo', 'Rabbit', 'Sheep', 'Mole', 'Pig']) ]\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter (animals [\"body\"], animals [ \"brain\"], color=\"blue\")\n",
    "from sklearn.linear_model import Linear Regression\n",
    "Lr =Linear Regression()\n",
    "lr.fit(X= animals [\"body\"].values.reshape(-1,1), y = animals [\"brain\"].values)\n",
    "\n",
    "brain_pred 1r.predict(X = animals [\"body\"].values.reshape(-1,1))\n",
    "plt.figure(figsize=(7, 5)) \n",
    "plt.scatter (animals [\"body\"], animals [\"brain\"], color=\"blue\") \n",
    "plt.plot(animals[\"body\"], brain_pred, color=\"red\", linewidth=2)\n",
    "\n",
    "new_animals_body= np.array([100, 200, 300, 400]) \n",
    "new_animals_brain = lr.predict(new_animals_body.reshape(-1,1)) \n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter (animals [\"body\"], animals [\"brain\"], color=\"blue\") \n",
    "plt.plot(animals [\"body\"], brain_pred, color=\"red\", linewidth=2) \n",
    "plt.scatter (new_animals_body, new_animals_brain, color = 'black', s=100)\n",
    "print(lr.coef_) \n",
    "print(1r.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### REGRESJA LINIOWA Z KLASYFIKACJĄ_KURS UDEMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "        #Load Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "        # Ładowanie danych\n",
    "iris = pd.read_csv(r\"C:\\data\\iris.data\",header = None,names = ['petal length', 'petal width', 'sepal length', 'sepal width', 'species'])\n",
    "iris.head()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "        # split data into features (X) and Labels (y)\n",
    "X= iris.iloc[:, :4]\n",
    "y = iris.loc[:, 'species']\n",
    "        # dictionary allowing to color points on diagrams \n",
    "categories = {'Iris-setosa' :1, 'Iris-versicolor' :2, 'Iris-virginica':3} \n",
    "y = y.apply(lambda x: categories [x])\n",
    "X.head()\n",
    "y.head()\n",
    "        #model is created here\n",
    "lr =LinearRegression() \n",
    "lr.fit(x,y)\n",
    "lr.score (x,y)\n",
    "\n",
    "        #some example data that need to be evaluated\n",
    "iris_1 = [5, 3.5, 1.4, 0.2] \n",
    "iris_2 [6.4, 3, 4.5, 1]\n",
    "iris_3= [6, 3, 5, 2]\n",
    "other = [1, 2, 3, 4]\n",
    "flowers =[iris_1, iris_2, iris_3, other]\n",
    "\n",
    "species_predict  = lr.predict(flowers)\n",
    "print(species_predict)\n",
    "\n",
    "        #replacing continous values into discrete values \n",
    "for f,s in zip (flowers, species_predict):\n",
    "        if round(s)==1:\n",
    "                print('Flower {} is {}'.format(f, 'Iris-setosa')) \n",
    "        elif round(s)== 2:\n",
    "                print('Flower {} is {}'.format(f, 'Iris-versicolor')) \n",
    "        elif round(s)== 3:\n",
    "                print('Flower {} is {}'.format(f, 'Iris-virginica')) \n",
    "        else:\n",
    "                print('Flower {} is {}'.format(f, 'UNKNOWN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Wariant modelu regresji z bublioteki STATSMODELS. API  - zaawansowany model OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24960/1592671894.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msm\u001b[0m               \u001b[1;31m#część biblioteki z api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlinearmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# OLS=ordinary list square method, tworzenie modelu regresji zaawansowanej\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinearmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                      \u001b[1;31m#res to result czyli wynik\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                              \u001b[1;31m# tabela z wynikami\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm               #część biblioteki z api\n",
    "linearmodel=sm.OLS(y,sm.add_constant(x))   # OLS=ordinary list square method, tworzenie modelu regresji zaawansowanej\n",
    "res=linearmodel.fit()                      #res to result czyli wynik\n",
    "res.summary()                              # tabela z wynikami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Przedziały ufności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#korzystamy z funkcji get_prediction dla przedziałów ufności\n",
    "\n",
    "xp= np.linspace(min(x), max(x),100)\n",
    "predictions = res.get_prediction(sm.add_constant(xp))    #przekazujemy ilość x dla których będzie liczone GET_PREDICTION przekazanie\n",
    "df=predictions.summary_frame(alpha=0.05)                 #przekazuje wartość alfa, frame pokazuje ramkę z info\n",
    "df\n",
    "plt.plot(xp,df['mean'].to_numpy())\n",
    "#przdziały ufnosci\n",
    "plt.plot(xp,df['mean_ci_lower'].to_numpy())             # mean_ci_lower\tmean_ci_upper skopiowane w tabeli powyżej\n",
    "plt.plot(xp,df['mean_ci_upper'].to_numpy())\n",
    "      # przedziały ufnosci mogą nas mniej interesować. ciekwasze są przedziały PREDYKCJI W ODSETKACH OBSERWACJI PRZYSZŁYCH- w ich obszarze powinien znaleźć sie 1-alfa\n",
    "      # przedział predyckji mówi czy dany obszar -gdzie?- jest 95% wszystkich predykcji\n",
    "plt.fill_between(xp,df['obs_ci_lower'].to_numpy(), df['obs_ci_upper'].to_numpy(),color='gray')\n",
    "plt.scatter(x,y,s=0.5)\n",
    "      #te szare pole pokrywa naszą funkcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Regresja wieloraka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "impoert seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2021)          # Określenie ziarna losowego\n",
    "\n",
    "N = 100 # liczba punktów     # Utworzenie sztucznych danych z szumem losowym\n",
    "x1 = st.norm(10,2).rvs(N)     # pierwsza kolumna\n",
    "x1 += np.random.rand(len(x))\n",
    "x2 = st.norm(1,2).rvs(N)     # druga kolumna\n",
    "x2 += np.random.rand(len(x))\n",
    "y = 3*x1 + 2*x2               #tworzone dane do takiej postaci\n",
    "y += st.norm(0,1.).rvs(N)    # szum - generuję taką samą ilość punktów losowych\n",
    "df = pd.DataFrame({\"x\":x,\"y\":y}) # tylko w celu  ładnego wypisania wartości-mamy tylko dwie kolumny x1 i x2\n",
    "df.head(5)\n",
    "sns.pairplot(df)\n",
    "plt.scatter(x,y)\n",
    "pass\n",
    "# W sformułowaniu macierzowym koeniczne jest dodanie wektora jedynek.Jest to sztuczna zmienna (stała), któa odpowiada wyrazowi wolnemu\n",
    "ones = np.ones_like(x,shape=(len(x),1))   # utworzenie pionowego wektora jedynek\n",
    "xv = x.reshape((-1,1))                    # utworzenie pionowego wektora x; transponujemy dataframe\n",
    "X = np.concatenate((ones,xv),axis=1)      # złączenie kolumn\n",
    "X\n",
    "st.linregress(x,y)\n",
    "a = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),y)  #Mnożenie macierzy w `numpy` można realizować za pomocą funkcji `np.dot(M1,M2)`T - trnspozycje\n",
    "a = np.linalg.inv(X.T@X)@X.T@y                          #lub za pomocą operatora `@`\n",
    "\n",
    "# Porównanie otrzymanego modelu i danych\n",
    "plt.scatter(x,y)\n",
    "t = np.arange(x.min(),x.max(),0.1)\n",
    "plt.plot(t,a[1]*t+a[0],\"red\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Współczynnik rozsiewu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "R = (df[['x1','x2']].corr()).to_numpy()  #wszystkie zmienne poza Y\n",
    "np.linalg.det(R)\n",
    "           # to co jest powyżej można zautomatyzować następująco:\n",
    "round(np.linalg.det(pd.DataFrame(X[:,1:]).corr().to_numpy)),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Zad1_advertising_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm             # z testami statystycznymi,które odpowiadają na pytania\n",
    "from sklearn.preprocessing import MinMaxScaler  # scikit daje same współczynniki, bez testów statystycznych\n",
    "import itertools as it\n",
    "\n",
    "df = pd.read_csv(\"advertising.csv\")\n",
    "feat = [\"TV\", \"Radio\",\"Newspaper\"]\n",
    "target = \"Sales\"\n",
    "         #najpierw robię model dla całości\n",
    "X1=sm.add_constant(df[df.columns[:-1]].to_numpy()) #stworzyłem iksy, utworzenie zmiennej\n",
    "y=df[\"Sales\"].to_numpy()                          #stworzyłem ygreki\n",
    "lm1= sm.OLS(y,X1).fit()                                  #stworzyłem model, sm to statsmodels   /  OLS ordinary list square do modeli liniowych\n",
    "lm1.summary()\n",
    "\n",
    "yp = lm.predict(X1) # w oryginale 04:50 wyeliminowano jadnazmienną(kolumnę) ale nic sie nie zmieniło co oznacza ze nespaper(reklamy w newspaper) nic nie wnosi dla biznesu\n",
    "plt.scatter(y,yp)\n",
    "_________________________________________________________________________________________\n",
    "x=df[\"TV\"].to_numpy()  #zaczynam od badania zależności, obserwuję wykres i podmieniam dla x jakieś równianie - potęga, mnożnik\n",
    "y-df[\"Sales\"].to_numpy()\n",
    "print(st.pearsonr(x,y))  #pojawia się dwa współczynniki, które będą poddawane potęgowaniu\n",
    "plt.scatter(x,y)         #>plt.scatter(np.sqrt(x),y)plt.scatter(x,y)  dało lepszy wykres dla liniowosci\n",
    "________________________________________________________________-\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=2022)\n",
    "X_train1=sm.add_constant(X_train)\n",
    "lm=sm.OLS(y_train, X_train1).fit()\n",
    "lm.summary()\n",
    "\n",
    "W = 0.6                                     # tw w to mnożnik, w oryginale było inaczej przemnażane\n",
    "X_train2 = sm.add_constant (X_train[:,:-1])\n",
    "X_train2[:,1] = X_train2 [:,1]**w\n",
    "X_test2 = sm.add_constant (X_test[:,:-1]) \n",
    "X_test2[:,1] = X_test2[:,1]**w \n",
    "lm2=sm.OLS (y_train,X_train2).fit()  \n",
    "lm2.summary()\n",
    "yp2=lm2.predict(sm.add_constant (X_test2))\n",
    "plt.scatter (y_test, yp2, color=\"blue\", s=0.75)\n",
    "______________________________________\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test,yp) #porównaie MSE na różncyh zbiorach\n",
    "mean_squared_error(y_test,yp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. ML3_REGRESJA LOGISTYCZNA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### ZADANIE Z DIABETES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "model = LogisticRegression().fit(x2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # estymator klasyfikacji=klasyfikator\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split # podział na zbiór testowy i treningowy\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, roc_auc_score,plot_roc_curve\n",
    "\n",
    "# biblioteki podstawowe\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix # podsumowanie klasyfikacji\n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score # ROC - diagnostyka modelu\n",
    "from sklearn.decomposition import PCA # analiza głównych składowych - w celu redukcji wymiarowości danych\n",
    "train_x, test_x, train_y, test_y = train_test_split(df.drop('Outcome', axis=1), df['Outcome']) # X to cała ramka danych,bez kolumny Outcome. Y będzie outcome\n",
    "#sprawdzam wielkości ramek, czy wszystko działa poprawnie\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "train_y.head() # upewniam się czy w zbiorze x nie ma y\n",
    "test_y.head()\n",
    "# najlepiej zacząć od najłatwiejszego modelu - logistycznego, z on daje regularyzację\n",
    "model = LogisticRegression().fit(train_x,train_y)\n",
    "# sprawdzam współczynniki dla X(kolumn) - interpretacja, jak dany czynnik wpływa na prawdopodobieństwo OR\n",
    "model.coef_\n",
    "# poniższe zmienne nie są wystandaryzowane, więc interpretacja jest następująca: 6 zmiennych działa pozytywnie czyli wzrost wartości zmiennej powoduje wzrost \n",
    "# wartości ryzyka prawdopodobieństwo zachorowania\n",
    "modelc.intercept_\n",
    "x_train.columns\n",
    "\n",
    "pred_train_proba = model.predict_proba(train_x)[:,1]\n",
    "pred_train_proba # zwraca prawdopod\n",
    "\n",
    "pred_train_proba = model.predict_proba(train_x)[:,1] # [:,1]aby była tylko 1 klasa, nie 0 i 1\n",
    "pred_train_proba                                     # otrzymujemy dwie wartości prawdopodobieństwa dla każdego wiersza, sumujące się do 1\n",
    "#kolejna metryka oceny jakości modelu \n",
    "accuracy_score(train_y,pred_train)\n",
    "#losowo wynik wyniósłby 65%, wyszło 77%\n",
    "confusion_matrix(train_y, pred_train) #ocenimy jak rozkłada się macierz pomyłek: 328 do przyporządkowanych zer, 119 dla jedynek, nie poprawnie sklasyfikowane 43 i 81\n",
    "# wyznaczam predykcję na zbiorze testowym\n",
    "pred_test = model.predict(test_x)\n",
    "pred_test_proba = model.predict_proba(test_x)[:,1]\n",
    "# pred_test_proba                    # otrzymujemy dwie wartości prawdopodobieństwa dla każdego wiersza, sumujące się do 1)\n",
    "\n",
    "#kolejna metryka oceny jakości modelu \n",
    "accuracy_score(test_y,pred_test)\n",
    "\n",
    "confusion_matrix(test_y, pred_test)\n",
    "         # krzywa ROC\n",
    "pred_test_proba = model.predict_proba(test_x)[:,1] # [:,1]aby była tylko 1 klasa, nie 0 i 1\n",
    "roc_auc_score(train_y,pred_train_proba) # razkład z wartości prawdopodob\n",
    "roc_auc_score(test_y,pred_test_proba) # razkład z wartości prawdopodobieństwa, nie wykorzystuje w AUC klas\n",
    "plot_roc_curve(model, train_x, train_y)  # skoki na wykresie to kolejne poziomy cut-off\n",
    "plot_roc_curve(model, test_x, test_y)  # skoki na wykresie to kolejne poziomy cut-off\n",
    "#gdyby pole pod krzywą wyszło większe niż na treningowym - underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ACCURACY na danych testowych\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Accuracy na 10-krotnej cross walidacji\n",
    "cv1 = cross_val_score(estimator=drzewo1, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
    "cv2 = cross_val_score(estimator=drzewo2, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
    "print(f'Cross-validated accuracy dla modelu pierwszego = {np.mean(cv1)}')\n",
    "print(f'Cross-validated accuracy dla modelu drugiego = {np.mean(cv2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ocena modelu i zbadanie przeuczenia\n",
    "\n",
    "model.intercept_\n",
    "model.coef_                   # każda kolumna ma nadany współczynnik\n",
    "x2_train.columns              # wypisuję kolumny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# na zbiorze treningowych\n",
    "pred_train_proba = model.predict_proba(x2_train)[:,1]         # predykcja na zbiorze treningowych\n",
    "pred_train_proba\n",
    "\n",
    "pred_train = model.predict(x2_train)\n",
    "pred_train\n",
    "\n",
    "# liczenie accuracy - pokazuje to matrycę 4 zmiennych, 2 i 3 pokazuje wartości źle przypisane\n",
    "accuracy_score(y2_train, pred_train)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(y2_train,pred_train)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# interpretacja wskaźnika jednej zmiennej\n",
    "roc_auc_score(y2_train, pred_train_proba)\n",
    "\n",
    "#wykres\n",
    "plot_roc_curve(model,x2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# na zbiorze testowym\n",
    "pred_train_proba1 = model.predict_proba(x2_test)[:,1]     # predykcja na zbiorze testowym\n",
    "pred_train_proba1\n",
    "\n",
    "pred_test2 = model.predict(x2_test)\n",
    "pred_test2\n",
    "\n",
    "# liczenie accuracy\n",
    "accuracy_score(y2_test, pred_test2)\n",
    "\n",
    "roc_auc_score(y2_test, pred_test_proba1)\n",
    "\n",
    "#wykres\n",
    "plot_roc_curve(model,x2_test,y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.4. ML4_DRZEWO DECYZYJNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1 Spróbujmy więc podzielić ten zestaw danych - wykorzystajmy wbudowaną w sklearn funkcję `train_test_split`\n",
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(iris.data, iris.target)\n",
    "train_data.shape, test_data.shape #domyślny stosunek 0,25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![CONDITIONALS](C:\\Users\\tk\\Desktop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2 budujemy obiekt klasyfikatora z kryterium ENTROPIA > są to hiperparametry które definiują ogólne zachowanie modelu\n",
    "clf = sklearn.tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "# dodanie kary w przypadku drzewa decyzyjnego - gdy nie chcemy dzielić drzewa gdy spadek nieczystości będzie poniżej jakiegoś progu\n",
    "clf= sklearn.tree.DecisionTreeClassifier(criterion='gini', min_impurity_decrease =)\n",
    "\n",
    "# 3 FIT trenuje nasz model, nakarmiamy go danymi ze zbiorów treningowych; Dopasowanie modelu dannych do funkcji fit\n",
    "clf.fit(train_data, train_target) # lub np. clf.fit(X_train, y_train)\n",
    "\n",
    "# 4 tworzenie wykresu drzewa\n",
    "plt.figure(figsize=(15, 12))\n",
    "_ = tree.plot_tree(clf, filled=True, class_names=iris.target_names, feature_names=iris.feature_names)\n",
    "\n",
    "# 5 sprawdzamy dokładność, w regresji było accuracy, tutaj score\n",
    "clf.score(test_data,test_target) #powinno być powyżej 0,9\n",
    "\n",
    "# 6 wyświetlanie danych klasyfikacji binarnych, bardziej dokładną analizę dokładności - wymaga to jednak \n",
    "#przeprowadzenia predykcji na danych testowych (gdyż funkcja generująca raport nic nie wie o modelu)\n",
    "predictions = clf.predict(test_data)\n",
    "print(sklearn.metrics.classification_report(test_target, predictions, target_names=iris.target_names))\n",
    "       # WYNIK: precision, recall(czułść), f1-score, suppoer(częstość występowania przykładów w zbiorze testowym)\n",
    "    \n",
    "# 7 gdzie popełniamy błąd? Można też zwizualizować w jaki sposób są rozmieszczone predykcje\n",
    "# get confusion matrix\n",
    "predictions = clf.predict(test_data)\n",
    "matrix = sklearn.metrics.confusion_matrix(test_target, predictions)\n",
    "\n",
    "# 8 wizualizacje błędów za  pomocą kolorów - pomaga aby nie pomylić osi\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(matrix,\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "           annot=True)\n",
    "plt.xlabel(\"Predicted labels\")  # it is important to validate labels\n",
    "plt.ylabel(\"True labels\")\n",
    "\n",
    "# 9 Użyteczną metryką jest f1_score - średnia harmoniczna precyzji i czułości\n",
    "# \"micro\" is preferrable, \"macro\" is good if there is no class imbalance\n",
    "sklearn.metrics.f1_score(test_target, predictions, average=\"micro\")  #micro -ustala w jaki sposób będziemy uśedniać po klasach\n",
    "\n",
    "# 10 REGULARYZACJA DRZEW- REDUKCJA GAŁĘZIASTOŚCI - opisane poniżej\n",
    "\n",
    "# 11 walidacja skrośna(krzyżowa)\n",
    "newer_regressor = DecisionTreeRegressor(max_depth=4)\n",
    "cv_results = cross_validate(newer_regressor, boston.data, boston.target, cv=10)  # this will result in R**2? ilość porcji na które chcmey podzielić zestaw danych\n",
    "         # SCORER pomaga nam ocenić wyniki, co jest lepsze a co gorsze. Wartości korelacji Pearsona zamienione na wartości błędu podniesionego do kwadratu\n",
    "scorer = sklearn.metrics.make_scorer(sklearn.metrics.mean_squared_error)  \n",
    "cv_results = cross_validate(newer_regressor, boston.data, boston.target, \n",
    "                            scoring=scorer, cv=10)  # this will result in mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# POWYŻSZA ANALIZA ZAWIERAŁA KLASYFIKATORA, PORA TERAZ NA REGRESORA\n",
    "# POJAWIA SIĘ TEŻ PROBLEM PRZYUCZANIA - OVERFITTING - przycinanie drzewa parametrem alpha ML4 01:35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drzewo1 = DecisionTreeClassifier(random_state=1)                   # odnosi się do danych z LOAD_wine      \n",
    "drzewo1.fit(X_train, y_train)\n",
    "pred1 = drzewo1.predict(X_test)\n",
    "acc1 = np.mean(y_test == pred1)                      from sklearn.metrics import accuracy_score\n",
    "\n",
    "drzewo2 = DecisionTreeClassifier(random_state=1, criterion='entropy', splitter='random', max_depth=5)\n",
    "drzewo2.fit(X_train, y_train)\n",
    "pred2 = drzewo2.predict(X_test)\n",
    "acc2 = np.mean(y_test == pred2)\n",
    "\n",
    "print(f'Accuracy dla modelu pierwszego = {acc1}')\n",
    "print(f'Accuracy dla modelu drugiego = {acc2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# PREDYKCJE NA ZBIORACH\n",
    "pred = clf.predict(X_test)\n",
    "np.mean(y_test.values == pred)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(class_weight={0: 1,1:5}, random_state=1)\n",
    "clf2.fit(X_train, y_train)\n",
    "pred = clf2.predict(X_test)\n",
    "np.mean(y_test.values == pred)\n",
    "confusion_matrix(y_test.values, pred)\n",
    "\n",
    "clf3 = DecisionTreeClassifier(class_weight='balanced', random_state=1)\n",
    "clf3.fit(X_train, y_train)\n",
    "pred = clf3.predict(X_test)\n",
    "np.mean(y_test.values == pred)\n",
    "confusion_matrix(y_test.values, pred)\n",
    "\n",
    "clf4 = DecisionTreeClassifier(class_weight='balanced', random_state=1, max_depth=5)\n",
    "clf4.fit(X_train, y_train)\n",
    "pred = clf4.predict(X_test)\n",
    "np.mean(y_test.values == pred)\n",
    "confusion_matrix(y_test.values, pred)\n",
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test.values, pred)                   # CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f1_score(y_test, pred)                                   # F1 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, pred)                               # recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, pred)                            # precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Counter(np.abs(clf4.tree_.feature)).most_common()        # LICZY ILE JEST PRZYPADKÓW z uzwględnieniem tree_.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "_ = tree.plot_tree(clf4, feature_names=features)         # RYSUNEK DRZEWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of nodes: \", reg.tree_.node_count)         # WYLICZAM LICZBĘ WĘZŁÓW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg.tree_.children_left          # sprawdzam próbki jakie zostały przypisane do lewego lub prawego drzewa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg.tree_.impurity       # sprawdzam nieczystość drzewa dla każdego z wierzchołków >> pod koniec powinny schodzić do wartosci 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg.score(tr_X, tr_y)  # współczynnik korealcji Pearsona  r**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###### Wytrenowany regressor jest olbrzymi. Oznacza to, że jest:\n",
    "    1. Zbyt dobrze dopasowany do danych\n",
    "    2. Zbyt skomplikowany do prostej interpretacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Regularyzacja drzew - REDUKUJEMY GAŁĘZIASTOŚĆ NA CZYTELNE, UPROSZCZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reg.get_params()         # badamy parametry drzewa\n",
    "new_regressor = DecisionTreeRegressor(\n",
    "    max_depth=4)         # HIPERPARAMETR:badamy jak drzewo sobie poradzi z ogranicznona max głębokością # IM WYŻSZA GŁĘBOKOŚĆ DRZEWA TYM BŁAD MNIEJSZY\n",
    "new_regressor.fit(tr_X, tr_y)\n",
    "plt.figure(figsize=(15, 10))\n",
    "_ = tree.plot_tree(new_regressor, filled=True, feature_names=boston.feature_names)\n",
    "\n",
    "train_predictions = new_regressor.predict(tr_X)\n",
    "predictions = new_regressor.predict(te_X)\n",
    "print(\"Error on training data: \", sklearn.metrics.mean_squared_error(tr_y, train_predictions))\n",
    "print(\"Error on test data: \", sklearn.metrics.mean_squared_error(te_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "         'splitter': ['best', 'random'],\n",
    "         'max_depth': [None, 1, 2, 5, 10],\n",
    "         'min_samples_split': [2, 5],\n",
    "         'min_samples_leaf': [1, 2, 3, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " # spróbujemy policzyć błąd predykcji?             za pomocą średniego błędu kwadratowego\n",
    "train_predictions = reg.predict(tr_X) \n",
    "predictions = reg.predict(te_X)\n",
    "print(\"Error on training data: \", sklearn.metrics.mean_squared_error(tr_y, train_predictions))\n",
    "print(\"Error on test data: \", sklearn.metrics.mean_squared_error(te_y, predictions)) \n",
    "print(\"Absolute on test data: \", sklearn.metrics.mean_absolute_error(te_y, predictions)) \n",
    " # przeciętnie mylimy się o + lub - 3,28 ale duza ilość predykcji odchyla błąd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf4, param_grid=params, verbose = 1, cv=10, scoring='accuracy')        # JAK W DANYCH POWYŻEJ ZAMIAST clf4 > drzewo1\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid.best_params_\n",
    "grid.best_score_\n",
    "grid.best_estimator_.fit(X_train, y_train)\n",
    "pred = grid.best_estimator_.predict(X_test)\n",
    "np.mean(y_test == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### ZADANIE<BR> Wyznacz najlepszą głębokość drzewa regresyjnego, korzystając z walidacji krzyżowej. Kryterium wyboru modelu powinno być oparte o metrykę błędu średniokwadratowego. Przetestuj zakres głębokości drzew od 3 do 6. Walidacja skrośna powinna być pięciokrotna (pięc podziałów na trening i test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mean_scores, max_scores = [], []        # tworzenie listy\n",
    "my_range= range (3, 20)\n",
    "\n",
    "for depth in my_range:\n",
    "    regressor=DecisionTreeRegressor (max_depth=depth)                                      # inicjalizacja modelu z odpowiednia głębokością\n",
    "    scorer = sklearn.metrics.make_scorer (sklearn.metrics.mean_squared_error)              # zrobienie obiektu scorera\n",
    "    results = cross_validate(regressor, boston.data, boston. target, scoring=scorer, cv=10)# użycie walidacji skrośnej na nowym modelu\n",
    "    mean_scores. append(results['test_score'].mean())                                      # podsumowanie wyników\n",
    "    max_scores.append(results['test_score'].max())\n",
    "\n",
    "plt.plot(my_range, mean_scores, label=\"Wartość średnia\")                                    # opis legendy\n",
    "plt.plot(my_range, max_scores, label=\"Wartość maksymalna\")                                 # opis legendy\n",
    "plt.ylim (0, 110)\n",
    "plt.xticks (my_range)\n",
    "plt.xlabel(\"Maksymalna głębokość drzewa\")                                                  # opis osi x\n",
    "plt.ylabel(\"Błąd średniokwadratowy\")                                                       # opis osi y\n",
    "plt.legend()\n",
    "plt.title(\"Błąd predykcji dla walidacji skrośnej o 10 paczkach\")                           # tytuł\n",
    "print(\"Optymalne ustawienie max_depth: \", 3+ np.array (mean_scores).argmin())              # funkcja argmin pozwala znaleść pozycję minimalną(odpowiadjącą max depth=3) - niebieski wykres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### ZADANIE Example 3 Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X = wine['data']\n",
    "y = wine['target']\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1 Spróbujmy więc podzielić ten zestaw danych - wykorzystajmy wbudowaną w sklearn funkcję `train_test_split`\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "# 2 budujemy obiekt klasyfikatora z kryterium ENTROPIA > są to hiperparametry które definiują ogólne zachowanie modelu\n",
    "# Accuracy na danych testowych\n",
    "drzewo1 = DecisionTreeClassifier(random_state=1)\n",
    "drzewo2 = DecisionTreeClassifier(random_state=1, criterion='entropy', splitter='random', max_depth=5)\n",
    "\n",
    "# 3 FIT trenuje nasz model, nakarmiamy go danymi ze zbiorów treningowych; Dopasowanie modelu dannych do funkcji fit\n",
    "drzewo1.fit(X_train, y_train)\n",
    "pred1 = drzewo1.predict(X_test)\n",
    "acc1 = np.mean(y_test == pred1) # lub np. clf.fit(X_train, y_train)\n",
    "\n",
    "drzewo2.fit(X_train, y_train)\n",
    "pred2 = drzewo2.predict(X_test)\n",
    "acc2 = np.mean(y_test == pred2)\n",
    "\n",
    "print(f'Accuracy dla modelu pierwszego = {acc1}')\n",
    "print(f'Accuracy dla modelu drugiego = {acc2}')\n",
    "\n",
    "# 4 tworzenie wykresu drzewa: tego w zadaniu nie było robionego\n",
    "plt.figure(figsize=(15, 12))\n",
    "_ = tree.plot_tree(clf, filled=True, class_names=iris.target_names, feature_names=iris.feature_names)\n",
    "\n",
    "# 5 sprawdzamy dokładność, w regresji było accuracy, tutaj score: tego w zadaniu nie było robionego\n",
    "clf.score(test_data,test_target) #powinno być powyżej 0,9\n",
    "\n",
    "# 6 wyświetlanie danych klasyfikacji binarnych, bardziej dokładną analizę dokładności - wymaga to jednak \n",
    "#przeprowadzenia predykcji na danych testowych (gdyż funkcja generująca raport nic nie wie o modelu): tego w zadaniu nie było robionego\n",
    "predictions = clf.predict(test_data)\n",
    "print(sklearn.metrics.classification_report(test_target, predictions, target_names=iris.target_names))\n",
    "       # WYNIK: precision, recall(czułść), f1-score, suppoer(częstość występowania przykładów w zbiorze testowym)\n",
    "    \n",
    "# 7 gdzie popełniamy błąd? Można też zwizualizować w jaki sposób są rozmieszczone predykcje\n",
    "# get confusion matrix : tego w zadaniu nie było robionego\n",
    "predictions = clf.predict(test_data)\n",
    "matrix = sklearn.metrics.confusion_matrix(test_target, predictions)\n",
    "\n",
    "# 8 wizualizacje błędów za  pomocą kolorów - pomaga aby nie pomylić osi\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(matrix,\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "           annot=True)\n",
    "plt.xlabel(\"Predicted labels\")  # it is important to validate labels\n",
    "plt.ylabel(\"True labels\")\n",
    "\n",
    "# 9 Użyteczną metryką jest f1_score - średnia harmoniczna precyzji i czułości\n",
    "# \"micro\" is preferrable, \"macro\" is good if there is no class imbalance\n",
    "sklearn.metrics.f1_score(test_target, predictions, average=\"micro\")  #micro -ustala w jaki sposób będziemy uśedniać po klasach\n",
    "\n",
    "# 10 REGULARYZACJA DRZEW- REDUKCJA GAŁĘZIASTOŚCI - opisane poniżej\n",
    "\n",
    "# 11 walidacja skrośna(krzyżowa)       # Accuracy na 10-krotnej cross walidacji\n",
    "cv1 = cross_val_score(estimator=drzewo1, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
    "cv2 = cross_val_score(estimator=drzewo2, X=X_train, y=y_train, scoring='accuracy', cv=10)\n",
    "print(f'Cross-validated accuracy dla modelu pierwszego = {np.mean(cv1)}')\n",
    "print(f'Cross-validated accuracy dla modelu drugiego = {np.mean(cv2)}')\n",
    "\n",
    "# SCORER pomaga nam ocenić wyniki, co jest lepsze a co gorsze(walidacja skrośna). Wartości korelacji Pearsona zamienione na wartości błędu podniesionego do kwadratu\n",
    "scorer = sklearn.metrics.make_scorer(sklearn.metrics.mean_squared_error)  \n",
    "cv_results = cross_validate(newer_regressor, boston.data, boston.target, \n",
    "                            scoring=scorer, cv=10)  # this will result in mean squared error\n",
    "#12 GridSearch\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "         'splitter': ['best', 'random'],\n",
    "         'max_depth': [None, 1, 2, 5, 10],\n",
    "         'min_samples_split': [2, 5],\n",
    "         'min_samples_leaf': [1, 2, 3, 5]}\n",
    "grid = GridSearchCV(drzewo1, param_grid=params, verbose = 1, cv=10, scoring='accuracy')  # trenuje parametry dla każdej kombnacji zmiennych \\ robimy instancję modelu\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "grid.best_score_\n",
    "grid.cv_results_  #sprawdzamy wyniki walidacji\n",
    "grid.best_estimator_.fit(X_train, y_train)\n",
    "pred = grid.best_estimator_.predict(X_test)\n",
    "np.mean(y_test == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 13 Regresja dla innego zbioru danych  REGRESSOR -podaje wartości konkretne a nie klasy. \n",
    "#Dąży do tego aby wartości były najbliższe średniej dzięki Mean squared error\n",
    "diab = load_diabetes()\n",
    "X_reg = diab['data']\n",
    "y_reg = diab['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=.2, random_state=1)\n",
    "\n",
    "reg_tree = DecisionTreeRegressor(random_state=1)\n",
    "reg_tree.fit(X_train, y_train)\n",
    "mse = np.mean((y_test - reg_tree.predict(X_test))**2)\n",
    "\n",
    "print(f'Mean squared error dla drzewa regresji to {mse}, a RMSE wynosi {np.sqrt(mse)}')\n",
    "\n",
    "np.abs(np.mean(cross_val_score(reg_tree, X_train, y_train, cv=10, scoring='neg_mean_squared_error', verbose=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.5.1. ML6_REGULARYZACJA _ LASSO; RIDGE; ELASTIC NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.3,random_state=42)  #test size to proporcja podziału zbioru\n",
    "\n",
    "ridge = Ridge(alpha=0.1, normalize=True) # alfa to stała, ona nakłada karę\n",
    "lasso = Lasso(alpha=0.1, normalize=True)\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio = 0.5, normalize=True)#ma dwa parametry - alfa i l1ratio\n",
    "\n",
    "lasso.fit(X_train,y_train)  lasso.fit(X_train,y_train).coef_ # do modelu wrzucamy dane treningowe - fitujemy \n",
    "        # predykcja na danych których model nie widział\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso_pred\n",
    "y_test # to są wartości które chcemy przewidziec\n",
    "\n",
    "#poniżej sprawdzamy jak daleko od rzeczywistych wartości sa wartości które wypredykowalismy  -SCORE dla problemów rekgresji\n",
    "ls = lasso.score(X_test,y_test)  #metoda domyślna do problemów regresji, współczynnik determinacji   r2  (metryka dopasowania)\n",
    "# im bliżej 1 to dopasowanie lepsze, lasso często nam zeruje wyniki, selekcja zmiennych, wybiera zmienne istotne dla modelu\n",
    "print(f'Lasso Score: {round(ls, 4)}')      #zaokrąglam do 4 miejsc po przecinku\n",
    "\n",
    "ridge.fit(X_train,y_train)  #wczesniej zaincjowaliśmy model, uczymy model i dopasowujemy wspócłczynniki\n",
    "           #predict on the test data\n",
    "ridge_pred= ridge.predict(X_test)\n",
    "            #get the score\n",
    "rs = ridge.score(X_test, y_test)\n",
    "print(f'Rodhe Score: {round (ls, 4)}')\n",
    "\n",
    "# store the fautre   sprawdzamy jak wygląda model po regualryzacji, (ściaganie do zera)\n",
    "names = diabetes.feature_names\n",
    "names\n",
    "diabetes = datasets.load_diabetes()\n",
    "                   # jak to zwizualizować LASSO - METODA SELEKCJI ZMIENNYCH-wybiera tylko te istotne\n",
    "lasso = Lasso(alpha=0.1) # im wyższy tym mniej zminnych zostaje, są sciagane do zera\n",
    "lasso.coef = lasso.fit(diabetes.data, diabetes.target).coef_  #to na końcu mówi o współczynnikach modelu liniowego | wybrane coef zerują się\n",
    "\n",
    "plt.plot(range(len(names)), lasso.coef) # na osi X tyle ile zmiennych, na Y współczynniki lasso.coef\n",
    "plt.xticks(range(len(names)),names,rotation=60)\n",
    "plt.ylabel('Coefficient')  #opis osi Y\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "names = diabetes.feature_names\n",
    "names\n",
    "diabetes = datasets.load_diabetes()\n",
    "ridge = Ridge(alpha=0.1) # im wyższy tym mniej zminnych zostaje, są sciagane do zera\n",
    "ridge.coef = ridge.fit(diabetes.data, diabetes.target).coef_  #to na końcu mówi o współczynnikach modelu liniowego, maja się wyzerowac te słabe\n",
    "ridge.coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 8.5.2. ML6_KROSWALIDACJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "reg = LinearRegression()   #dataset irys\n",
    "cv_results = cross_val_score(reg,diabetes.data, diabetes.target,cv=5)  #ilokrotność walidacji cv(razy: trening na 4, testowanie na jednej |, reg to jest model\n",
    "print(f'Scores: {np.round(cv_results,3)}')\n",
    "print(f'Scores mean: {np.round(np.mean(cv_results),3)}') #liczę średnią ze scorów z kroswalidacji\n",
    "reg.fit(diabetes.data, diabetes.target)\n",
    "reg.score(diabetes.data, diabetes.target)  #SCORE Z DANYCH I TARGETU , MODEL JEST FITOWANY DO TYCH DANYCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. ML6_RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 5, criterion = 'gini', max_depth=4, bootstrap=True, random_state=1) \n",
    "# pierwszy klasyfikator z kryterium podziału, coot true -chcemy z próbami losowo wyciagane z populacji                                                                                                   # random po to by zawsze uzyskać ten sam wynik przy losowości(reprodukowalność wyników), czesto jest 42\n",
    "rf_classifier.fit(X_train, y_train)  # dopasowujemy model do danych treningowych\n",
    "\n",
    "rf_classifier.score(X_test, y_test)    # score dał nam wysoki wynik, stosunek obserwacji zaklasyfikowano poprawnie jako 1, błędnie jako 0 - czyli trafiło\n",
    "\n",
    "rf_classifier.estimators_  #pokazuje wszystkie skonstruowane drezwa decyzyjne\n",
    "rf_classifier.estimators_[0]   # wybieramy dostęp do konkretnego drzewa, wytrenowanego klasyfikatira\n",
    "\n",
    "plt.figure(figsize = (15,12))\n",
    "\n",
    "plot_tree(rf_classifier.estimators_[0],\n",
    "          feature_names=iris.feature_names,   #kolorowanie kolumn-ficzerów\n",
    "          class_names=iris.target_names,\n",
    "         filled=True);                     #ważny jest ten średnik\n",
    "plt.figure(figsize = (15,12))\n",
    "plot_tree(rf_classifier.estimators_[1],\n",
    "          feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names,\n",
    "         filled=True,);\n",
    "# Lasy losowe pozwalają na uzyskanie oszacowania istotności każdej ze zmiennych.\n",
    "import numpy as np\n",
    "feature_importances = pd.DataFrame(rf_classifier.feature_importances_, index=iris.feature_names,       # ważność, metoda feature_importavne określa istoność zmiennych, liczba wystąpień zmiennych w danych podziałach\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "feature_importances          #sumuje się do jedynki\n",
    "np.sum(rf_classifier.feature_importances_)\n",
    "rf_classifier.feature_importances_  #dostajemy wartości które sa istotne w predykcji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7. ML_PCA -algorytm redukcji wymiarów,odkrycie prawidłowości między cechami,nowe nieskorelowane zmienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_23164/3815487900.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\tk\\AppData\\Local\\Temp/ipykernel_23164/3815487900.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    * *n_components* - liczba *n* czynników w nowej przestrzeni\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# n_components* - liczba *n* czynników w nowej przestrzeni \n",
    "#svd_solver* - typ dekompozycji macierzy. Dostępne wartości: *auto, full, arpack, randomized*\n",
    "\n",
    "# kolumny do wykluczenia (te na których nie chcemy PCA)\n",
    "exclude_filter = ~df.columns.isin(['Nazwa'])  #WYRZUCAMY KOLUMNE NAZWA\n",
    "# liczba głównych składowych\n",
    "pca = PCA(n_components = 3)                 #TRZY GŁÓWNE SKŁADOWE W RAMACH ALGORYTMU\n",
    "# przeliczenie\n",
    "principal_components = pca.fit_transform(df.loc[:, exclude_filter])  #SKLEARN - FIT_TRANSFROM NA RAMCE DANYCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_df = pd.DataFrame(data = principal_components, \n",
    "                           columns = ['principal component 1',\n",
    "                                      'principal component 2',\n",
    "                                      'principal component 3'])\n",
    "principal_df['Nazwa'] = df['Nazwa']\n",
    "principal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from chart_studio import plotly as py\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True) #ABY MOC WYŚWIETLAĆ OBRAZKI W RAMACH NOTATNIKA\n",
    "trace0 = go.Scatter(\n",
    "    x = principal_df['principal component 1'],       #RYSUJE SCATTERPPLOT DLA WÓCH WYBRANYCH SKŁADOWYCH X Y\n",
    "    y = principal_df['principal component 2'],\n",
    "    text=principal_df['Nazwa'],\n",
    "    textposition=\"top center\",\n",
    "    name = 'Piony',\n",
    "    mode = 'markers+text',\n",
    "    marker = dict(\n",
    "        size = 10,\n",
    "        color = 'rgb(228,26,28)',\n",
    "        line = dict(\n",
    "            width = 1,\n",
    "            color = 'rgb(0, 0, 0)')))\n",
    "data = [trace0]\n",
    "layout = dict(title = 'Podobieństwo Banków na podstawie PCA',\n",
    "              yaxis = dict(zeroline = False, title ='PC2 (principal component 2)'),\n",
    "              xaxis = dict(zeroline = False, title ='PC1 (principal component 1)')\n",
    "             )\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='styled-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                   PCA - jak dobrać liczbę komponentów? \n",
    "# Pierwszym ze sposób jest wizualizacja skumulowanej wartości wariancji w zależności od liczby komponentów. \n",
    "#Z wykresu można  odczytać, że 4 pierwszych czynników może wyjaśnić ponad 95% całkowitej wariancji.  \n",
    "import numpy as np\n",
    "\n",
    "pca = PCA().fit(df.loc[:, exclude_filter])\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))  #CUMSUM TO SKUMULOWANE SUMY\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "#                   Wykorzystanie parametru svd_solver.\n",
    "\n",
    "#Jeżeli **0 < *n_components* < 1** oraz ***svd_solver = 'full'*** to funkcja PCA wybiera liczbę komponentów, \n",
    "#tak aby wielkość wariancji, którą należy wyjaśnić, była większa niż procent określony przez *n_components*.\n",
    "pca = PCA(svd_solver='full', n_components=0.95)              # SOLVER USTAWIONY NA FULL, WARTOŚCI MNIEJSZE OD 1 ON OTRAKTUJE JAKO PROCENT ZMIENNOŚCI, WARIANCJA NIE MNIEJSZA NIZ 95%\n",
    "\n",
    "principal_components = pca.fit_transform(df.loc[:, exclude_filter])\n",
    "principal_df = pd.DataFrame(data=principal_components)\n",
    "principal_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. ML7_XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "# KC House Data\n",
    "df = pd.read_csv('./kc_house_data.csv')\n",
    "df_train = df[['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'waterfront', 'view', 'grade', 'lat', 'yr_built', 'sqft_living15']]\n",
    "\n",
    "X = df_train.values\n",
    "y = df.price.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Fitting XGB regressor model and default base learner is Decision Tree\n",
    "xgb_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=75, subsample=0.75, max_depth=7)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Making Predictions\n",
    "predictions = xgb_reg.predict(X_test)\n",
    "\n",
    "# Variance_score\n",
    "print((explained_variance_score(predictions, y_test)))\n",
    "\n",
    "# To convert data table into a matrix\n",
    "kc_dmatrix = xgb.DMatrix(data=X, label=y, feature_names=df_train)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\": \"reg:linear\", \"max_depth\": 2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=kc_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree as num_trees = 0 and features importance\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "xgb.plot_importance(xg_reg)\n",
    "__________________________________________________________________________\n",
    "# Linear Base Learner\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Convert the training and testing sets into DMatrixes\n",
    "boston_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "boston_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Parameters with booster as gblinear for Linear base learner\n",
    "params = {\"booster\": \"gblinear\", \"objective\": \"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=boston_train, num_boost_round=5)\n",
    "\n",
    "# Making predictions\n",
    "predictions = xg_reg.predict(boston_test)\n",
    "\n",
    "# Computing RMSE\n",
    "print(\"RMSE: %f\" % (np.sqrt(mean_squared_error(y_test, predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.9. ML8_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapowanie etykiet, tak byśmy potem mogli skorzystać z rysowania wykresów decyzyjnych plot_decision_regions\n",
    "encoder = LabelEncoder()\n",
    "data['Kolor'] = encoder.fit_transform(data['Kolor'])\n",
    "# uczenie modelu stadarodow jak innych w pakiecie sklearn\n",
    "# wybieramy kernel liniowy by pokazać podział linią prostą - o krenelach będzie w dalszej części \n",
    "simple_svm = SVC(kernel='linear')\n",
    "simple_svm.fit(data[['X', 'Y']], data['Kolor']);\n",
    "\n",
    "X = pd.DataFrame(df[0], columns=['a', 'b'])\n",
    "y = df[1]\n",
    "svm = SVC(kernel='linear').fit(X, y)\n",
    "plot_decision_regions(X = X.to_numpy(), y = y, clf=svm);\n",
    "\n",
    "# rysowanie wykresu decyzyjności na przetrzeni \n",
    "plot_decision_regions(X = data[['X', 'Y']].to_numpy(), y = data['Kolor'].to_numpy().astype(np.int), clf=simple_svm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8. ML8_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df, columns=['alcohol', 'sulphates'])\n",
    "y = df['quality']\n",
    "sns.scatterplot(X.alcohol, X.sulphates, hue = y);\n",
    "\n",
    "lin_ovo = SVC(kernel='linear', gamma='auto', C=100)\n",
    "lin_ovo.fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y.to_numpy(), clf=lin_ovo);\n",
    "\n",
    "lin_ovr = OneVsRestClassifier(SVC(kernel='linear')).fit(X,y)\n",
    "plot_decision_regions(X.to_numpy(),y.to_numpy(), clf=lin_ovr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "697e03dbff18c2aea5427d36c45620e2702223449f86e35f030338dd12b34fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
